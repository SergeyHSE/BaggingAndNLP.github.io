# -*- coding: utf-8 -*-
"""Task6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16brJfyEDQb3AM2lcRGOy7rQRAUEChtAQ

### –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è

1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ `load_wine` –∏–∑ `sklearn.datasets`. –ò–∑ –¥–∞–Ω–Ω—ã—Ö –∏—Å–∫–ª—é—á–∏—Ç–µ –æ–±—ä–µ–∫—Ç—ã –∫–ª–∞—Å—Å–∞ 2.
 –û—Ç–º–∞—Å—à—Ç–∞–±–∏—Ä—É–π—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∫–ª–∞—Å—Å `StandardScaler` —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é. –û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∏ –æ—Ü–µ–Ω–∏—Ç–µ –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –£–∫–∞–∂–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –æ–∫–∞–∑–∞–ª—Å—è –Ω–∞–∏–º–µ–Ω–µ–µ –∑–Ω–∞—á–∏–º—ã–º.

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ª–µ–∂–∏—Ç –ø–æ –∫–ª—é—á—É `'target'`, –º–∞—Ç—Ä–∏—Ü–∞ –æ–±—ä–µ–∫—Ç—ã-–ø—Ä–∏–∑–Ω–∞–∫–∏ –ª–µ–∂–∏—Ç –ø–æ –∫–ª—é—á—É `'data'`
"""

from sklearn.datasets import load_wine
data = load_wine()
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
pd.set_option('display.max_columns', None)

print(data)


X, y = data['data'], data['target']

y = np.expand_dims(y, axis=1)
y.shape

data1 = np.concatenate((X, y), axis=1)
data1

#data2 = np.delete(data1, np.where(data1[:, -1] == 2), axis=0)

data_new = data1[data1[:, -1] !=2]

X_cut = np.delete(data_new, 13, axis=1)
X_cut.shape

y_cut = np.delete(data_new, np.s_[0:13], axis=1)
y_cut.shape

y_target = np.squeeze(y_cut, axis=-1)
y_target.shape

X_data = pd.DataFrame(data=X_cut, columns=data.feature_names)
X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, shuffle=False)

scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaler = scaler.transform(X_train)
clf_skaler = LogisticRegression()
clf_skaler.fit(X_train_scaler, y_train)
weight_sorted = sorted(zip(clf_skaler.coef_.ravel(), data.feature_names), reverse=True)
weights_scaler = [x[0] for x in weight_sorted]
features_scaler = [x[1] for x in weight_sorted]
df_scaler = pd.DataFrame({'features_scaler':features_scaler, 'weights_scaler':weights_scaler})

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 6)) 
df_scaler.plot.barh(x='features_scaler', y='weights_scaler', color='skyblue', legend=False, ax=ax)
plt.title('Feature Weights (Scaled)')
plt.xlabel('Weight')
plt.ylabel('Features')
plt.gca().invert_yaxis()
plt.grid(axis='x', linestyle='--', alpha=0.7)
for i, v in enumerate(df_scaler['weights_scaler']):
    ax.text(v + 0.01, i, f'{v:.2f}', va='center', fontsize=10)
plt.tight_layout()
plt.savefig('output.png', dpi=300)
plt.show()

"""2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ `load_wine` –∏–∑ `sklearn.datasets`. –ò–∑ –æ–±—É—á–∞—é—â–µ–π —á–∞—Å—Ç–∏ –∏—Å–∫–ª—é—á–∏—Ç–µ –æ–±—ä–µ–∫—Ç—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Å—É 2. –ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–π—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. –û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–æ–º—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å.

"""

clf = LogisticRegression()
clf.fit(X_train, y_train)
clf.coef_
weight_table = sorted(zip(clf.coef_.ravel(), data.feature_names), reverse=True)
weight_table
weights = [x[0] for x in weight_table]
features = [x[1] for x in weight_table]
df = pd.DataFrame({'features':features, 'weights':weights})
ax = df.plot.barh(x='features', y='weights', rot=0,)


"""3. –†–µ—à–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –î–∞–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ –æ–±—ä–µ–∫—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ ùëã –∏ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤  ùë¶. –û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∏ –ø—Ä–µ–¥—Å–∫–∞–∂–∏—Ç–µ –∫–ª–∞—Å—Å –æ–±—ä–µ–∫—Ç–∞ x_new"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X1 = np.array([[1, 1], [0.3, 0.7], [0, 4], [-2, -7], [0, -2], [-1, -1], [-2, 0]])
y1 = np.array([1, 1, 1, 0, 0, 0, 0])
X_new = np.array([[-5, 1]])

X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.1)
y_test1
X_test1
logreg = LogisticRegression()
logreg.fit(X_train1, y_train1)
X_new_pred = logreg.predict_proba(X_new)
X_new_pred
score = accuracy_score(X_new_pred, y_test1)
score

"""### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤

4. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª SMSSpamCollection –∏–∑ UCI (https://archive.ics.uci.edu/ml/machine-learning-databases/00228/). –î–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –±–∏–Ω–∞—Ä–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (‚Äòspam‚Äô, ‚Äòham‚Äô), –ü—É—Å—Ç—å –≤ –æ–±—É—á–∞—é—â—É—é —á–∞—Å—Ç—å –ø–æ–ø–∞–¥—É—Ç –ø–µ—Ä–≤—ã–µ 4000 –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã, –≤ —Ç–µ—Å—Ç–æ–≤—É—é —á–∞—Å—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –æ–±—ä–µ–∫—Ç—ã. –û–±—É—á–∏—Ç–µ `TfidfVectorizer` —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö –∏–∑ –æ–±—É—á–∞—é—â–µ–π —á–∞—Å—Ç–∏ –∏ –ø–æ–ª—É—á–∏—Ç–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π —á–∞—Å—Ç–∏. –£–∫–∞–∂–∏—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

–ß—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ, —Å–∫–∞—á–∞–π—Ç–µ —Ñ–∞–π–ª –ø–æ —Å—Å—ã–ª–∫–µ. –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ google colab, —Ç–æ –ø—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–≤–µ–¥–µ–Ω –Ω–∏–∂–µ.
"""
import pandas as pd
import numpy as np
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import tqdm
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore")

path = r"C:\Users\User\Documents\pyton-projects\spider\–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\logRegression\spam.csv"
print(path.replace("\\", "/"))

text = pd.read_csv('C:/Users/User/Documents/pyton-projects/spider/–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ/logRegression/spam.csv', encoding='latin-1')
text.head()
text.columns
text = text.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)
text.head()
text = text.rename(columns={'v1':'target', 'v2':'data'})
text.head()
text.shape

def bow(vectorizer, train, test):
    train_bow = vectorizer.fit_transform(train)
    test_bow = vectorizer.transform(test)
    return train_bow, test_bow
    
massege = text['data']
y = text['target']

X_train = massege.iloc[:4000]
X_train.tail()
y_train = y.iloc[:4000]
y_train.shape
X_test = massege.iloc[4000:]
X_test.shape
y_test = y.iloc[4000:]

#X_train, X_test, y_train, y_test = train_test_split(massege, y, test_size=0.28212491, shuffle=False)

X_train.shape
X_test = X_test.reset_index(drop=True)
X_test.shape
X_test.head()
y_test = y_test.reset_index(drop=True)
y_test.shape

def preprocess_text(texts):
    stop_words = set(stopwords.words('english'))
    regex = re.compile('[^a-z A-Z]')
    preprocess_texts = []
    for i in tqdm.tqdm(range(len(texts))):
        text = texts[i].lower()
        text = regex.sub(' ', text)
        word_tokens = word_tokenize(text)
        filtered_sentence = [w for w in word_tokens if not w in stop_words]
        preprocess_texts.append( ' '.join(filtered_sentence))
        
    return preprocess_texts


X_train_preprocess = preprocess_text(X_train)
X_test_preprocess = preprocess_text(X_test)

from nltk.stem.lancaster import LancasterStemmer

def stemming_texts(texts):
    st = LancasterStemmer()
    stem_text = []
    for text in tqdm.tqdm(texts):
        word_tokens = word_tokenize(text)
        stem_text.append(' '.join([st.stem(word) for word in word_tokens]))
    return stem_text

X_train_stemming = stemming_texts(X_train_preprocess)
X_test_stemming = stemming_texts(X_test_preprocess)

vectorizer_tfidf = TfidfVectorizer()

X_train_bow, X_test_bow = bow(vectorizer_tfidf, X_train_preprocess, X_test_preprocess)
X_train_bow.shape
X_test_bow.shape

"""5.  –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª SMSSpamCollection –∏–∑ UCI (https://archive.ics.uci.edu/ml/machine-learning-databases/00228/). –î–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –±–∏–Ω–∞—Ä–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (‚Äòspam‚Äô, ‚Äòham‚Äô), –ü—É—Å—Ç—å –≤ –æ–±—É—á–∞—é—â—É—é —á–∞—Å—Ç—å –ø–æ–ø–∞–¥—É—Ç –ø–µ—Ä–≤—ã–µ 4000 –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã, –≤ —Ç–µ—Å—Ç–æ–≤—É—é —á–∞—Å—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –æ–±—ä–µ–∫—Ç—ã. –û–±—É—á–∏—Ç–µ `TfidfVectorizer`, –ø–æ–º–∏–º–æ —Å–ª–æ–≤ –≤—Ö–æ–¥—è—â–∏—Ö –≤ —Ç–µ–∫—Å—Ç—ã, —É—á–∏—Ç—ã–≤–∞–π—Ç–µ –±–∏–≥—Ä–∞–º–º—ã (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä `ngram_range`). –£–∫–∞–∂–∏—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""

vectorizer_ngram = TfidfVectorizer(ngram_range=(1, 2))
X_train_ngram, X_test_ngram = bow(vectorizer_ngram, X_train_preprocess, X_test_preprocess)
X_train_ngram.shape
X_test_ngram.shape



"""6. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª SMSSpamCollection –∏–∑ UCI (https://archive.ics.uci.edu/ml/machine-learning-databases/00228/). –î–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –±–∏–Ω–∞—Ä–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (‚Äòspam‚Äô, ‚Äòham‚Äô), –ü—É—Å—Ç—å –≤ –æ–±—É—á–∞—é—â—É—é —á–∞—Å—Ç—å –ø–æ–ø–∞–¥—É—Ç –ø–µ—Ä–≤—ã–µ 4000 –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã, –≤ —Ç–µ—Å—Ç–æ–≤—É—é —á–∞—Å—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –æ–±—ä–µ–∫—Ç—ã. –û–±—É—á–∏—Ç–µ `TfidfVectorizer`, –Ω–µ —É—á–∏—Ç—ã–≤–∞–π—Ç–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏—Å—å –º–µ–Ω—å—à–µ 2 —Ä–∞–∑ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä `min_df`). –£–∫–∞–∂–∏—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""

vectorizer_min_df = TfidfVectorizer(min_df=2)
X_train_df, X_test_df = bow(vectorizer_min_df, X_train_preprocess, X_test_preprocess)
X_train_df.shape
X_test_df.shape

"""7. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª SMSSpamCollection –∏–∑ UCI (https://archive.ics.uci.edu/ml/machine-learning-databases/00228/). –î–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –±–∏–Ω–∞—Ä–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (‚Äòspam‚Äô, ‚Äòham‚Äô), –ü—É—Å—Ç—å –≤ –æ–±—É—á–∞—é—â—É—é —á–∞—Å—Ç—å –ø–æ–ø–∞–¥—É—Ç –ø–µ—Ä–≤—ã–µ 4000 –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã, –≤ —Ç–µ—Å—Ç–æ–≤—É—é —á–∞—Å—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –æ–±—ä–µ–∫—Ç—ã. –û–±—É—á–∏—Ç–µ `TfidfVectorizer` —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö –∏–∑ –æ–±—É—á–∞—é—â–µ–π —á–∞—Å—Ç–∏ –∏ –ø–æ–ª—É—á–∏—Ç–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π —á–∞—Å—Ç–∏. –ù–∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö –æ–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∏ –æ—Ü–µ–Ω–∏—Ç–µ –¥–æ–ª—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π —á–∞—Å—Ç–∏. –£–∫–∞–∂–∏—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–æ–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤."""

from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(X_train_bow, y_train)
y_redict = clf.predict(X_test_bow)
accuracy = accuracy_score(y_redict, y_test)
print('Logistic accuracy: ', accuracy)
